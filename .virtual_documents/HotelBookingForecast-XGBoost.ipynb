import pandas as pd
import xgboost as xgb
import numpy as np
import warnings
warnings.filterwarnings('ignore')


booking = pd.read_csv('hotel_bookings_clean.csv')
booking.info()


booking.head()


booking.describe()


booking['is_canceled'].value_counts().plot(kind='bar')


booking['is_canceled'].value_counts()/booking['is_canceled'].count() * 100


booking.corr()


booking.corr()['is_canceled'].sort_values(ascending=False)


X, y = booking.iloc[:, 1:], booking.iloc[:, 0]


from sklearn.model_selection import train_test_split


X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size=.33,
                                                    shuffle=True)


xgb_clf = xgb.XGBClassifier()
xgb_clf.get_params()


from sklearn.model_selection import RandomizedSearchCV

rs_param_grid = {
    'max_depth': list((range(3,12))),
    'alpha': [0,0.001, 0.01,0.1,1],
    'subsample': [0.5,0.75,1],
    'learning_rate': np.linspace(0.01,0.5, 10),
    'n_estimators': [10, 25, 40]
    }

xgb_clf = xgb.XGBClassifier(random_state=123)
xgb_rs = RandomizedSearchCV(estimator=xgb_clf,param_distributions=rs_param_grid, 
                                cv=3, n_iter=5, verbose=2, random_state=123)
xgb_rs.fit(X_train, y_train)
print("Best parameters found: ", xgb_rs.best_params_)
print("Best accuracy found: ", xgb_rs.best_score_)


xgb_clf.set_params(n_estimators=10)
xgb_clf.fit(X_train, y_train)
pred = xgb_clf.predict(X_test)


accuracy = np.sum(pred==y_test)/y_test.shape[0]
accuracy


import matplotlib
matplotlib.rcParams['figure.figsize'] = (10, 8)


xgb.plot_importance(xgb_clf)


!pip install graphviz


matplotlib.rcParams['figure.figsize'] = (15, 15)
xgb.plot_tree(xgb_clf, num_trees=0)


from sklearn import metrics
print(metrics.classification_report(y_test, pred))


# optimized for both memory efficiency and training speed
booking_dmatrix = xgb.DMatrix(data=X, label=y)


parms = {'objective':'binary:logistic', 'max_depth':3}
xgb_cv = xgb.cv(dtrain=booking_dmatrix,
                params=parms, 
                nfold=3,
                num_boost_round=40,
                seed=123,
                metrics=['error'],
                early_stopping_rounds=10 #If the validation performance does not improve for 10 consecutive rounds, training is stopped early.

               )
xgb_cv.head()


accuracy = (1 - xgb_cv['test-error-mean'].iloc[-1]) * 100
accuracy


from sklearn.metrics import accuracy_score
xgb_clf = xgb.XGBClassifier(n_estimator=25)


xgb_clf.set_params(max_depth=10, subsample=0.5)
xgb_clf.fit(X_train, y_train)
pred = xgb_clf.predict(X_test)
accuracy_score(y_test, pred)


xgb_clf.set_params(colsample_bytree=0.5)
xgb_clf.fit(X_train, y_train)
pred = xgb_clf.predict(X_test)
accuracy_score(y_test, pred)



